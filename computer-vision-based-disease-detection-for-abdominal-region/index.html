<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  

 <title>Computer Vision-based Disease Detection for the Abdominal Region</title>



 <meta property="og:locale" content="en-US">
  <meta property="og:site_name" content="Salih Enes Metin">
  <meta property="og:title" content="Computer Vision-based Disease Detection for the Abdominal Region">
  
  
  <link rel="canonical" href="https://tutenstein.github.io/computer-vision-based-disease-detection-for-abdominal-region/">
  <meta property="og:url" content="https://tutenstein.github.io/computer-vision-based-disease-detection-for-abdominal-region/">
  
  
    <script type="application/ld+json">
      {
        "@context": "http://schema.org",
        "@type": "Person",
        "name": "Salih Enes Metin",
        "url": "https://tutenstein.github.io",
        "sameAs": null
      }
      </script>
  
  <link href="https://tutenstein.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Salih Enes Metin feed">
  <meta content="A study on the use of artificial intelligence (AI) in medical image analysis is discussed in the project. Challenges in using AI for medical image analysis, including the need for large amounts of high-quality data and the complexity of the diagnostic process, are mentioned." name="description">
  <meta content="Deep learning, abdomen, medical image analysis, artificial intelligence" name="keywords">

  <!-- Favicons -->
  <link rel="shortcut icon" type="image/x-icon" href="../assets/img/1679219322.ico">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio
  * Updated: Mar 10 2023 with Bootstrap v5.2.3
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->

</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="../assets/img/salih-enes.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Salih Enes Metin</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://www.linkedin.com/in/salih-enes-metin/" target="_blank" rel="noopener noreferrer" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/tutenstein" target="_blank" rel="noopener noreferrer" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://medium.com/@salih.metin" target="_blank" rel="noopener noreferrer" class="medium"><i class="bx bxl-medium"></i></a>
          <a href="#" class="instagram"><i class="bx bxl-instagram"></i></a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li><a href="../" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
          <li><a href="../#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i> <span>CV</span></a></li>
          <li><a href="../#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Projects</span></a></li>
          <li><a href="../#blog" class="nav-link scrollto"><i class="bx bxs-archive"></i> <span>Blog</span></a></li>
          <li><a href="../#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>

        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Computer Vision-based Disease Detection for the Abdominal Region</h2>
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Project Details</li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">

        <div class="row gy-4">

          <div class="col-lg-8">
            <div class="section-title">
              <p><strong>Summary</strong></p>

              <p>&nbsp; &nbsp;A study on the use of artificial intelligence (AI) in medical image analysis is discussed in the project. Challenges in using AI for medical image analysis, including the need for large amounts of high-quality data and the complexity of the diagnostic process, are mentioned. A dataset provided by the Ministry of Health, which included CT scan images from 1,182 patients, containing a total of 357,405 image slices, was used in the study. The images were divided into 17 different classes, each representing a different health condition. The number of classes was reduced to seven by combining some and removing others.</p>
              <br>
              <p>&nbsp; &nbsp;Convolutional neural networks (CNNs) were used to analyze the images and classify them into their respective health conditions. The CNNs were trained on a subset of the dataset and tested on the remaining images. The authors report that their approach achieved high accuracy in classifying the images, outperforming previous studies on the same dataset.</p>
              <br>
              <p>&nbsp; &nbsp;It is suggested that their approach could be used to assist radiologists in diagnosing health conditions more accurately and efficiently. However, it is also cautioned that further research is needed to validate the approach and ensure its safety and effectiveness in clinical settings.</p>
              <br>
              <p><strong>1. Methods</strong></p>
              
              <ol>
                <li><strong>Yolo</strong><br />
                &nbsp; &nbsp; In our research on YOLO architecture, we found that this architecture is a method that uses convolutional neural networks (CNNs). One of the important features of convolutional neural networks is reducing the number of parameters, which drew our attention to this method. The reduction in parameters causes the model to produce results faster, making the YOLO architecture one of the models we can use well. The YOLO architecture provides us with speed, high success rate, and excellent learning capacity. Additionally, with the bounding box regression and intersection over union (IoU)-based loss function in YOLOv4, the learning capacity of the model improves. As a result, it has been determined that the model will be successful in classification and localization.</li>
                <li><strong>Resnet</strong><br />
                &nbsp; &nbsp;In our research on ResNet architecture, we realized that it is quite difficult to train very deep neural networks due to the problem of vanishing/exploding gradients. However, the ResNet building block structure allows us to create deeper layer neural networks, solving this problem. Therefore, we decided to use ResNet in our model, thinking that it would increase the model&#39;s success by going deeper and extracting more features.</li>
              </ol>
              
              <p><strong>2. Approches&nbsp;</strong></p>
              <br>
              <p>&nbsp; &nbsp; When we reduced the dataset provided to us by the Ministry of Health to a size that could be processed in memory, we noticed that many of the features of the tomography image were lost. We investigated a method of using a &quot;data generator&quot; class, which would allow us to read the images while they were in motion when they were used for training. By reading the images while they were in motion, we saved memory. We customized this class structure by adding various functions such as image preprocessing layers and converting the tomography image to an array by using DICOM.<br />
                <br>
                &nbsp; &nbsp;We had a difficult time deciding between the YOLOv4 and ResNet models we were considering using. Both models had similar success rates. We had read in several articles before that another model was used as a backbone in YOLO and then YOLO classification was done. Based on our tests and observations, we discovered that the ResNet50 model was a good feature extractor. As a result, we decided to use a YOLO model with ResNet50 as the backbone. We created the model architecture again by using the ResNet50 model in the part of the YOLO model that extracts features. We can see this in Figure I.</p>
              <div class="img-with-text">
                <img src="../assets/img/project/yolo_resnet.png" alt="" class="responsive-img">
                <p><em><strong>Figure I:</strong> Illustration of the part where ResNet will be added on the YOLO model[3]</em></p>
              </div>
              <p>&nbsp; &nbsp;After completing the model architecture, we changed the default parameters on the model to make it better able to recognize the training set given to us. Instead of using Tanh and sigmoid functions for activation functions, we used a combination of ReLU and Mish functions. This allowed us to avoid the problem of vanishing gradients that can occur in deep-layer models. While ReLU reduced the computational complexity and made the optimization process much faster, Mish increased the model&#39;s information storage and discriminative capacity. [4] The reason for this is that Mish has negative derivatives at some points and positive derivatives at others, rather than having all positive or all negative derivatives. [5]</p>
              <div class="figure-2">
              <div class="img-with-text">
                <img src="../assets/img/project/figure_2.png" alt="">
                <p><em><strong>Figure II:</strong> Graphs of activation functions</em></p>
            </div>
          </div>
          <div class="figure-3">
            <div class="img-with-text">
              <img src="../assets/img/project/figure3.png" alt="">
              <p><em><strong>Figure III:</strong> Graphs of Mish activation functions</em></p>
          </div>
          </div>
          <p><strong>3. Results and Review</strong></p>
          <p>&nbsp; &nbsp;There are three successful approaches of the ResNet model, which are ResNet50, ResNet101, and ResNet152. In the given data, only labeled data was randomly split into an 80-20 ratio. The split data was tested with 50 epochs, batch size of 64, categorical cross-entropy as the loss function, and Rmsprop as the optimization algorithm. The results of the tests are shown in Table 1.</p>
          <div class="figure-3">
            <div class="img-with-text">
              <img src="../assets/img/project/table-1.png" alt="">
              <p><em><strong>Table I:</strong> Comparison of ResNet models on different success metrics</em></p>
          </div>
          </div>
          <br>
          <p>&nbsp; &nbsp;Upon examining Table 1, it is observed that the model that performed the best was ResNet50. The highest training success rate we achieved from the models was calculated as 96.4%. The success metrics of the ResNet50 model, which achieved the best performance, are shown in the training graph in Figure IV, and the model&#39;s test results and table are shown in Figure V.</p>
          <div class="figure-2">
            <div class="img-with-text">
              <img src="../assets/img/project/figure-5.png" alt="">
              <p><em><strong>Figure IV:</strong> Training graph of the performance metrics of the ResNet50 model</em></p>
          </div>
          </div>
          <div class="figure-3">
          <div class="img-with-text">
            <img src="../assets/img/project/figure-4.png" alt="">
            <p><em><strong>Figure V:</strong> Test results and table of the ResNet50 model</em></p>
        </div>
      </div>
          <p>&nbsp; &nbsp;Different variations of ResNet, from the original to fully pre-activated, were used on ResNet50 so that the gradients can continue to flow without being blocked by shortcut connections to any previous layers [6]. The reason for using this approach is to enable the model to extract more features without memorization. Subsequently, we removed the classifier layers of the ResNet50 model, making it a pure feature extractor. This approach has been supported by many articles, stating that ResNet is a successful feature extractor. Then, the YOLOV4 classifier layers were used to complete the model. To combine these two models, we used the open-source neural network framework called darknet, written in the C language.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-6.png" alt="" class="responsive-img">
              <p><em><strong>Figure VI:</strong> Different residual block variants of ResNet</em></p>
          </div>
          <p>&nbsp; &nbsp;The YOLOV4 classification layer was connected to the last convolutional layer of the ResNet50 model. We used the loss function of YOLOv4, which is based on classification, confidence, and IOU. This enabled the model to be trained more effectively for both classification and localization.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-7.png" alt="" class="responsive-img">
              <p><em><strong>Figure VII:</strong> YOLOv4 loss function</em></p>
          </div>
          <p>&nbsp; &nbsp;After completing the architecture of our model with Resnet50 backbone and YOLOv4, the labeled data provided to us for model training was gathered by only taking the bounding box values from the Excel file. Later, the MBB coordinates were rescaled and adapted to the model for YOLO.</p>
          <br>
          <p>&nbsp; &nbsp;The organized data was divided into 80-20 for training and testing respectively. Additionally, since there was no label of &#39;0&#39; (no detection) in the organized data file, a portion of the test data was added with no detections in a 1/7 ratio.</p>
          <br>
          <p>&nbsp; &nbsp;When selecting hyperparameters for YOLOv4, various tests were performed in many different scenarios. The learning rate parameter was the most important parameter in these tests. Tests were performed with a learning rate of 0.1, then with 0.01, and finally with 0.01, 0.001, and 0.0001 for different training tests. After these tests, we decided that the most suitable parameter for our model was 0.001. Afterwards, we decided on the other parameters based on our research. The hyperparameter selection stage will continue until the day of the competition, and tests will be continuously performed during this process, and our model will be constantly updated. After determining the hyperparameters in our model, we proceeded to the training phase. The best weights during the model&#39;s training were saved every 1000 iterations. The loss and mAP graphs for the model can be seen in Figure VIII.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-8.png" alt="" class="responsive-img">
              <p><em><strong>Figure VIII:</strong> YOLOv4 training mAP and training loss values</em></p>
          </div>

          <p>&nbsp; &nbsp;After training the model, the best weights obtained were tested and the mAP result we obtained was 80.73%. The detailed values of the test performed can be seen in Figure IX.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-9.png" alt="" class="responsive-img">
              <p><em><strong>Figure IX:</strong> YOLOv4 training mAP and training loss values</em></p>
          </div>
          <p><strong>4.The datasets used in the experimentation and training phases </strong></p>
          <br>
          <p>&nbsp; &nbsp;Initially, we thoroughly examined the data provided to us by the Ministry of Health. The dataset provided by the Ministry of Health includes the computed tomography (CT) images of 1,182 individuals. There are a total of 357,405 slices in these images, of which 38,236 slices are marked and 5,348 consist of start/end slices. The start/end slices do not have coordinates.</p>
          <br>
          <p>&nbsp; &nbsp;The first shared dataset contains 17 different classes. Each of these classes contains a different number of CT slices. The abdominal aortic aneurysm class has 9009 slices, the acute pancreatitis class has 7121 slices, the acute appendicitis class has 5991 slices, the acute cholecystitis class has 5153 slices, the kidney stone class has 2042 slices, the gallbladder stone class has 1684 slices, the acute diverticulitis class has 1152 slices, the kidney-bladder class has 876 slices, the abdominal aorta class has 852 slices, the abdominal aortic dissection class has 814 slices, the ureter stone class has 726 slices, the colon class has 688 slices, the pancreas class has 632 slices, the gallbladder class has 594 slices, the appendix class has 468 slices, the appendicolith class has 359 slices, and the calcified diverticulum class has 75 slices.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-10.png" alt="" class="responsive-img">
              <p><em><strong>Figure X:</strong> Data set distribution graph</em></p>
          </div>
          <p>&nbsp; &nbsp;As there are too many classes in this dataset, which could complicate the procedures we will perform, we made some changes to the dataset. We removed some classes and merged some others. The organized dataset contains 36,119 marked slices, of which 5,191 are start/end slices. This dataset contains 7 different classes. The &quot;no finding&quot; class has 4111 slices, the acute appendicitis class has 5991 slices, the acute cholecystitis class has 5153 slices, the acute pancreatitis class has 7121 slices, the kidney stone class has 2768 slices, the acute diverticulitis class has 1152 slices, and the aortic aneurysm class contains 9823 slices.</p>
            <div class="img-with-text">
              <img src="../assets/img/project/figure-11.png" alt="" class="responsive-img">
              <p><em><strong>Figure XI:</strong> Edited dataset graph</em></p>
          </div>
          </div>
          </div>


          <div class="col-lg-4">
            <div class="portfolio-info">
              <h3>Project information</h3>
              <ul>
                <li><strong>Project Name</strong>: Computer Vision-based Disease Detection for the Abdominal Region</li>
                <li><strong>Project Category</strong>: Deep Learning</li>
                <li><strong>Project date</strong>: 01 July, 2022</li>
              </ul>
            </div>
          </div>

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>iPortfolio</span></strong>
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/ -->
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../assets/vendor/aos/aos.js"></script>
  <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="../assets/vendor/typed.js/typed.min.js"></script>
  <script src="../assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="../assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="../assets/js/main.js"></script>

</body>

</html>